---
title: "CENPA_ChIP_stylo"
output: html_document
---

```{r setup, include=FALSE}
library(stylo)
library(ggpubr)
library(ggplot2)
```

Save first column of dataframe with reads as txt file 
Rank READSCORE then READCOUNT
```{r cars}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore")
B6 <- read.table(file = "B6_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", header = TRUE)
CAST <- read.table(file = "CAST_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", header = TRUE)
LEWES <- read.table(file = "LEWES_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", header = TRUE)
PWK <- read.table(file = "PWK_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", header = TRUE)
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo")
write.table(B6[,1], file = "STYLO_B6_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(CAST[,1], file = "STYLO_CAST_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(LEWES[,1], file = "STYLO_LEWES_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(PWK[,1], file = "STYLO_PWK_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)

```
Save first column of dataframe with reads as txt file 
Rank READCOUNT then READSCORE
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore")
B6 <- read.table(file = "B6_0.1percent_allTRUE_normReadscore_normReadcount_top1000.txt", header = TRUE)
CAST <- read.table(file = "CAST_0.1percent_allTRUE_normReadscore_normReadcount_top1000.txt", header = TRUE)
LEWES <- read.table(file = "LEWES_0.1percent_allTRUE_normReadscore_normReadcount_top1000.txt", header = TRUE)
PWK <- read.table(file = "PWK_0.1percent_allTRUE_normReadscore_normReadcount_top1000.txt", header = TRUE)
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo/")
write.table(B6[,1], file = "STYLO_B6_0.1percent_allTRUE_normReadscore_normReadcount_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(CAST[,1], file = "STYLO_CAST_0.1percent_allTRUE_normReadscore_normReadcount_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(LEWES[,1], file = "STYLO_LEWES_0.1percent_allTRUE_normReadscore_normReadcount_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(PWK[,1], file = "STYLO_PWK_0.1percent_allTRUE_normReadscore_normReadcount_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)

```

Create a stylo.corpus object by reading in files from a directory
the files in the directory should be the text files you want to compare with computational stylistics methods
Comparing full reads
```{r}
top1000.corpus <- load.corpus(files = "all", 
                              corpus.dir = "/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_raw",
                              encoding = "UTF-8")

```


#########
FEATURES
#########
Then, we would like to extract gaugeable features from the corpus
Features are typically extracted at the level of words (word tokens)
or characters (n-grams) - basically kmers
Stylo selects partially overlapping series of character groups of length n from words

In our data, we have 1000 "words" from each strain and we would like to compare their 76-gram space 
This is to assess the relationship between the strains CENP-A words
```{r}
corpus.char.76.grams <- txt.to.features(top1000.corpus, 
                                        ngram.size = 76,
                                        features = "c")

```


These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(corpus.char.76.grams)

frequency.list.value <- make.frequency.list(corpus.char.76.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(corpus.char.76.grams, features = frequency.list)
```


Analysis of features
Defaults to a standard cluster analysis of the 100 most frequent words
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_n76/")
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      pca.visual.flavour = "technical",
      write.png.file = TRUE, 
      gui = FALSE)
```

This code chunk doesn't work, not sure why it cannot make a bootstrap consensus tree.....?
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_n76")
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
      write.png.file = TRUE, 
      gui = FALSE)
```

output plot in Rstudio panel so I can save it as an SVG
```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 8000, mfw.max = 8000,
      gui = FALSE)
```

Output with the consensus, with mfw.min and mfw.max specified as 8000
to make sure it is using all reads
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_n76/with_consensus/all")
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 8000, mfw.max = 8000,
      pca.visual.flavour = "technical",
      write.png.file = TRUE, 
      gui = FALSE)
```

Remove PCA visual flavor to get just datapoints without reads
```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 8000, mfw.max = 8000,
      gui = FALSE)
```


```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 8000, mfw.max = 8000,
      pca.visual.flavour = "symbols",
      colors.on.graphs = c("grey","grey","green","green","purple","purple","yellow","blue","red","red"),
      gui = FALSE)
```

make min most frequent words equal to 4000 and max most frequent words equal to 8000? 

```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 4000, mfw.max = 8000,
      gui = FALSE)
```



#########
FEATURES
#########
SO we tried 76 grams (characters)
But, i think it was counting spaces as a character
let's look at n-grams with grams being words and n = 1

```{r}
corpus.words.1.grams <- txt.to.features(top1000.corpus, 
                                        ngram.size = 1,
                                        features = "w")

```

These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(corpus.words.1.grams)

frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)
```
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_w1")
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 5329, mfw.max = 5329,
      write.png.file = TRUE, 
      gui = FALSE)
```

##Try to add this and see if this visual flavor works
assign.plot.colors(sample.names, col = "greyscale")
```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      pca.visual.flavour = "symbols",
      mfw.min = 5329, mfw.max = 5329,
      colors.on.graphs = "black",
      write.png.file = TRUE, 
      gui = FALSE)

```

```{r}

stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 5329, mfw.max = 5329,
      gui = FALSE)
```

```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
      mfw.min = 4329, mfw.max = 5329,
      write.png.file = TRUE, 
      gui = FALSE)
```


#########
FEATURES
#########
ANALYSIS with the lists of enriched 31 mers!

```{r}
kmers.corpus <- load.corpus(files = "all", 
                              corpus.dir = "/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_kmers_raw",
                              encoding = "UTF-8")

```

```{r}
kmers.words.1.grams <- txt.to.features(kmers.corpus, 
                                        ngram.size = 1,
                                        features = "w")

```

These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(kmers.words.1.grams)

frequency.list.value <- make.frequency.list(kmers.words.1.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(kmers.words.1.grams, features = frequency.list)
```

```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_k31")
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 10000, mfw.max = 10000,
      write.png.file = TRUE, 
      gui = FALSE)
```


```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 10000, mfw.max = 10000,
      gui = FALSE)
```


```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
      mfw.min = 9000, mfw.max = 10000,
      write.png.file = TRUE, 
      gui = FALSE)
```


##########################
Y centromere stylo analysis with enriched reads
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/Y_cent")
B6 <- read.table(file = "B6_Yinf_reads.fastq", header = FALSE)
B6.new = B6[seq(2, nrow(B6), 4), ]
B6.new <- as.data.frame(B6.new)

CAST <- read.table(file = "CAST_Yinf_reads.fastq", header = FALSE)
CAST.new = CAST[seq(2, nrow(CAST), 4), ]
CAST.new <- as.data.frame(CAST.new)

LEWES <- read.table(file = "LEWES_Yinf_reads.fastq", header = FALSE)
LEWES.new = LEWES[seq(2, nrow(LEWES), 4), ]
LEWES.new <- as.data.frame(LEWES.new)

PWK <- read.table(file = "PWK_Yinf_reads.fastq", header = FALSE)
PWK.new = PWK[seq(2, nrow(PWK), 4), ]
PWK.new <- as.data.frame(PWK.new)

setwd("/projects/dumont-lab/uma/CENPA_ChIP/Y_cent/stylo/YinfReads")
write.table(B6.new, file = "B6_Yinf_reads.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
write.table(CAST.new, file = "CAST_Yinf_reads.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
write.table(LEWES.new, file = "LEWES_Yinf_reads.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)
write.table(PWK.new, file = "PWK_Yinf_reads.txt", row.names = FALSE, col.names = FALSE, quote = FALSE)

```
Create a stylo.corpus object by reading in files from a directory
the files in the directory should be the text files you want to compare with computational stylistics methods
Comparing full reads
```{r}
Yinf.corpus <- load.corpus(files = "all", 
                           corpus.dir = "/projects/dumont-lab/uma/CENPA_ChIP/Y_cent/stylo/YinfReads",
                           encoding = "UTF-8")

```

SO we tried 76 grams (characters)
But, i think it was counting spaces as a character
let's look at n-grams with grams being words and n = 1

```{r}
corpus.words.1.grams <- txt.to.features(Yinf.corpus, 
                                        ngram.size = 1,
                                        features = "w")

```

These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(corpus.words.1.grams)

frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)
```
##NOTE: this plot is changing a LOT by changinf mfw.min and mfw.max
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/Y_cent/stylo/YinfPlots")
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 100, mfw.max = 3496,
      write.png.file = TRUE, 
      gui = FALSE)
```

```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 100, mfw.max = 3496,
      gui = FALSE)
```

```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
      mfw.min = 100, mfw.max = 3496,
      write.png.file = TRUE, 
      gui = FALSE)
```



Y centromere stylo analysis with SPAdes assembled contigs
Create a stylo.corpus object by reading in files from a directory
the files in the directory should be the text files you want to compare with computational stylistics methods
Comparing full reads
```{r}
SPAdes.corpus <- load.corpus(files = "all", 
                           corpus.dir = "/projects/dumont-lab/uma/CENPA_ChIP/Y_cent/stylo/SPAdesContigs",
                           encoding = "UTF-8")

```

SO we tried 76 grams (characters)
But, i think it was counting spaces as a character
let's look at n-grams with grams being words and n = 1

```{r}
corpus.words.1.grams <- txt.to.features(SPAdes.corpus, 
                                        ngram.size = 1,
                                        features = "w")

```

These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(corpus.words.1.grams)

frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)
```

```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/Y_cent/stylo/SPAdesContigsPlots")
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 100, mfw.max = 100,
      write.png.file = TRUE, 
      gui = FALSE)
```





```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/Y_cent/stylo/SPAdesContigsPlots")
stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 100, mfw.max = 100,
      write.png.file = TRUE, 
      gui = FALSE)

```

```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/Y_cent/stylo/SPAdesContigsPlots")
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
      mfw.min = 10, mfw.max = 160,
      write.png.file = TRUE, 
      gui = FALSE)
```


########################11/2/21##########################

Make dummy data to test out stylo

```{r}
test.corpus <- load.corpus(files = "all", 
                              corpus.dir = "/fastscratch/uma/stylo_test",
                              encoding = "UTF-8")

```


#########
FEATURES
#########
Extract gaugeable features from the corpus
Features are typically extracted at the level of words (word tokens)
or characters (n-grams) - basically kmers
Stylo selects partially overlapping series of character groups of length n from words

```{r}
corpus.word.1.grams <- txt.to.features(test.corpus, 
                                        ngram.size = 1,
                                        features = "w")

```


These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(corpus.word.1.grams)

frequency.list.value <- make.frequency.list(corpus.word.1.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(corpus.word.1.grams, features = frequency.list)
```


Analysis of features
Defaults to a standard cluster analysis of the 100 most frequent words
```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      pca.visual.flavour = "technical",
      gui = FALSE)
```


output plot in Rstudio panel so I can save it as an SVG
```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 30, mfw.max = 30,
      gui = FALSE)
```

Output with the consensus, with mfw.min and mfw.max specified as 8000
to make sure it is using all reads
```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 30, mfw.max = 30,
      pca.visual.flavour = "technical",
      gui = FALSE)
```

Remove PCA visual flavor to get just datapoints without reads
```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 30, mfw.max = 30,
      gui = FALSE)
```

```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
      mfw.min = 10, mfw.max = 40,
      gui = FALSE)
```


Adding Iwata Otsubo data

Save first column of dataframe with reads as txt file 
Rank READSCORE then READCOUNT
```{r cars}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore")
B6 <- read.table(file = "B6_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", header = TRUE)
CAST <- read.table(file = "CAST_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", header = TRUE)
LEWES <- read.table(file = "LEWES_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", header = TRUE)
PWK <- read.table(file = "PWK_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", header = TRUE)
B6Iwata <- read.table(file = "B6Iwata_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
CHPO <- read.table(file = "CHPO_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)

setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo1READSCORE2READCOUNT")
write.table(B6[,1], file = "STYLO_B6_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(CAST[,1], file = "STYLO_CAST_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(LEWES[,1], file = "STYLO_LEWES_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(PWK[,1], file = "STYLO_PWK_0.1percent_allTRUE_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(B6Iwata[,1], file = "STYLO_B6Iwata_0.1percent_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(CHPO[,1], file = "STYLO_CHPO_0.1percent_normReadcount_normReadscore_top1000.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)

```


Create a stylo.corpus object by reading in files from a directory
the files in the directory should be the text files you want to compare with computational stylistics methods
Comparing full reads
```{r}
top1000.corpus <- load.corpus(files = "all", 
                              corpus.dir = "/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo1READSCORE2READCOUNT",
                              encoding = "UTF-8")

```


let's look at n-grams with grams being words and n = 1

```{r}
corpus.words.1.grams <- txt.to.features(top1000.corpus, 
                                        ngram.size = 1,
                                        features = "w")

```

These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(corpus.words.1.grams)

frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)
```

```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 5606, mfw.max = 5606,
      gui = FALSE)
```


```{r}

stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 5606, mfw.max = 5606,
      gui = FALSE)
```

```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
       mfw.min = 3876, mfw.max = 5606,
      write.png.file = TRUE, 
      gui = FALSE)
```

Classify?


12/09/21
Replicates stylo analysis
Prepare txt files
```{r cars}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/replicates")
B61 <- read.table(file = "B6_F_092920_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
B62 <- read.table(file = "B6_F_092920_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)
B63 <- read.table(file = "B6_M_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
B64 <- read.table(file = "B6_M_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)

CAST1 <- read.table(file = "CAST_F_011921_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
CAST2 <- read.table(file = "CAST_F_011921_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)
CAST3 <- read.table(file = "CAST_M_011921_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
CAST4 <- read.table(file = "CAST_M_011921_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)
CAST5 <- read.table(file = "CAST_M_101620_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
CAST6 <- read.table(file = "CAST_M_101620_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)

LEWES1 <- read.table(file = "LEWES_F_011921_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
LEWES2 <- read.table(file = "LEWES_F_011921_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)
LEWES3 <- read.table(file = "LEWES_F_012621_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
LEWES4 <- read.table(file = "LEWES_F_012621_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)
LEWES5 <- read.table(file = "LEWES_M_011921_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
LEWES6 <- read.table(file = "LEWES_M_011921_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)

PWK1 <- read.table(file = "PWK_F_012621_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
PWK2 <- read.table(file = "PWK_F_012621_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)
PWK3 <- read.table(file = "PWK_F_092920_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
PWK4 <- read.table(file = "PWK_F_092920_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)
PWK5 <- read.table(file = "PWK_M_101620_CENPA_0.1percent_normReadcount_normReadscore_top1000.txt", header = TRUE)
PWK6 <- read.table(file = "PWK_M_101620_CENPA_0.1percent_normReadscore_normReadcount_top1000.txt", header = TRUE)

setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_replicates/joint_kmers")
write.table(B61[,1], file = "B6_F_092920_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(B62[,1], file = "B6_F_092920_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(B63[,1], file = "B6_M_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(B64[,1], file = "B6_M_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)

write.table(CAST1[,1], file = "CAST_F_011921_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(CAST2[,1], file = "CAST_F_011921_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(CAST3[,1], file = "CAST_M_011921_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(CAST4[,1], file = "CAST_M_011921_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(CAST5[,1], file = "CAST_M_101620_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(CAST6[,1], file = "CAST_M_101620_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)

write.table(LEWES1[,1], file = "LEWES_F_011921_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(LEWES2[,1], file = "LEWES_F_011921_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(LEWES3[,1], file = "LEWES_F_012621_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(LEWES4[,1], file = "LEWES_F_012621_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(LEWES5[,1], file = "LEWES_M_011921_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(LEWES6[,1], file = "LEWES_M_011921_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)


write.table(PWK1[,1], file = "PWK_F_012621_CENPA_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(PWK2[,1], file = "PWK_F_012621_CENPA_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(PWK3[,1], file = "PWK_F_092920_CENPA_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(PWK4[,1], file = "PWK_F_092920_CENPA_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(PWK5[,1], file = "PWK_M_101620_CENPA_COUNT_SCORE.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)
write.table(PWK6[,1], file = "PWK_M_101620_CENPA_SCORE_COUNT.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)

```
RUN STYLO - only with joint kmers
Create a stylo.corpus object by reading in files from a directory
the files in the directory should be the text files you want to compare with computational stylistics methods
Comparing full reads
```{r}
top1000.corpus <- load.corpus(files = "all", 
                              corpus.dir = "/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_replicates/joint_kmers",
                              encoding = "UTF-8")

```


let's look at n-grams with grams being words and n = 1

```{r}
corpus.words.1.grams <- txt.to.features(top1000.corpus, 
                                        ngram.size = 1,
                                        features = "w")

```

These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(corpus.words.1.grams)

frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)
```


```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 9318, mfw.max = 9318,
      gui = FALSE)
```


```{r}

stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 9318, mfw.max = 9318,
      gui = FALSE)
```



```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
       mfw.min = 7318, mfw.max = 9318,
      write.png.file = TRUE, 
      gui = FALSE)
```


################################################################


RUN STYLO - with combined replicates and joint kmer replicates
```{r}
top1000.corpus <- load.corpus(files = "all", 
                              corpus.dir = "/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_replicates/combined_joint_kmers",
                              encoding = "UTF-8")

```


let's look at n-grams with grams being words and n = 1

```{r}
corpus.words.1.grams <- txt.to.features(top1000.corpus, 
                                        ngram.size = 1,
                                        features = "w")

```

These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(corpus.words.1.grams)

frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)
```


```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 1473, mfw.max = 10473,
      gui = FALSE)
```


```{r}

stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 10473, mfw.max = 10473,
      gui = FALSE)
```


```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
       mfw.min = 1473, mfw.max = 10473,
      write.png.file = TRUE, 
      gui = FALSE)
```


#################################
3/10/22

How likely is the pattern we see due to chance?
Take a random sets of 1000 sequences from strains
and perform PCA and CA analysis using stylo
Note down PCA coordinates and CA distances
Perform this 1000 times
See how value compares with distribution

Loop to run stylo 

```{r}

for (i in 1:1){

   corpusdirectory <- paste("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_randomization/rep", i, sep = "")
   
   top1000.corpus <- load.corpus(files = "all", 
                              corpus.dir = corpusdirectory,
                              encoding = "UTF-8")
   corpus.words.1.grams <- txt.to.features(top1000.corpus, 
                                        ngram.size = 1,
                                        features = "w")
   frequency.list <- make.frequency.list(corpus.words.1.grams)
   frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)
   frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)

   PCA <- stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 4004, mfw.max = 4004,
      gui = FALSE)
   
   PCAcoord <- as.data.frame(PCA$pca.coordinates)
   PCAcoord$STRAIN <- rownames(PCAcoord)
   PCAcoord %>% 
      pivot_longer(-STRAIN, names_to = "sample", values_to = "PCaxis") -> PCAcoord
   
   PCAcoord$DistMinor <- 0
   PCAcoord[1:6,4] <- PCAcoord[1:6,3] - PCAcoord[25:30,3]
   PCAcoord[7:12,4] <- PCAcoord[7:12,3] - PCAcoord[25:30,3]
   PCAcoord[13:18,4] <- PCAcoord[13:18,3] - PCAcoord[25:30,3]
   PCAcoord[19:24,4] <- PCAcoord[19:24,3] - PCAcoord[25:30,3]
   PCAcoord[25:30,4] <- PCAcoord[25:30,3] - PCAcoord[25:30,3]
   PCAcoord[31:36,4] <- PCAcoord[31:36,3] - PCAcoord[25:30,3]
   
   
   PCAcoord$DistMajor <- 0
   PCAcoord[1:6,5] <- PCAcoord[1:6,3] - PCAcoord[19:24,3]
   PCAcoord[7:12,5] <- PCAcoord[7:12,3] - PCAcoord[19:24,3]
   PCAcoord[13:18,5] <- PCAcoord[13:18,3] - PCAcoord[19:24,3]
   PCAcoord[19:24,5] <- PCAcoord[19:24,3] - PCAcoord[19:24,3]
   PCAcoord[25:30,5] <- PCAcoord[25:30,3] - PCAcoord[19:24,3]
   PCAcoord[31:36,5] <- PCAcoord[31:36,3] - PCAcoord[19:24,3]
   
   PCAname <- paste("PCA", i, sep = "")
   assign(PCAname, PCAcoord)
   
   CA <- stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 4004, mfw.max = 4004,
      gui = FALSE)

   CAdist <- as.data.frame(CA$distance.table)
   CAdist$STRAIN <- rownames(CAdist)
   CAdist %>% 
      pivot_longer(-STRAIN, names_to = "sample", values_to = "dist") -> CAdist
   
   CAdist <- CAdist[c(2:6,9:12,16:18,34,35),]
   
   CAname <- paste("CA", i, sep = "")
   assign(CAname, CAdist)
   
}
```


```{r}
for (i in 2:1000){

   corpusdirectory <- paste("/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_randomization/rep", i, sep = "")
   
   top1000.corpus <- load.corpus(files = "all", 
                              corpus.dir = corpusdirectory,
                              encoding = "UTF-8")
   corpus.words.1.grams <- txt.to.features(top1000.corpus, 
                                        ngram.size = 1,
                                        features = "w")
   frequency.list <- make.frequency.list(corpus.words.1.grams)
   frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)
   frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)

   PCA <- stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 4004, mfw.max = 4004,
      gui = FALSE)
   
   PCAcoord <- as.data.frame(PCA$pca.coordinates)
   PCAcoord$STRAIN <- rownames(PCAcoord)
   PCAcoord %>% 
      pivot_longer(-STRAIN, names_to = "sample", values_to = "PCaxis") -> PCAcoord
   
   PCAcoord$DistMinor <- 0
   PCAcoord[1:6,4] <- PCAcoord[1:6,3] - PCAcoord[25:30,3]
   PCAcoord[7:12,4] <- PCAcoord[7:12,3] - PCAcoord[25:30,3]
   PCAcoord[13:18,4] <- PCAcoord[13:18,3] - PCAcoord[25:30,3]
   PCAcoord[19:24,4] <- PCAcoord[19:24,3] - PCAcoord[25:30,3]
   PCAcoord[25:30,4] <- PCAcoord[25:30,3] - PCAcoord[25:30,3]
   PCAcoord[31:36,4] <- PCAcoord[31:36,3] - PCAcoord[25:30,3]
   
   
   PCAcoord$DistMajor <- 0
   PCAcoord[1:6,5] <- PCAcoord[1:6,3] - PCAcoord[19:24,3]
   PCAcoord[7:12,5] <- PCAcoord[7:12,3] - PCAcoord[19:24,3]
   PCAcoord[13:18,5] <- PCAcoord[13:18,3] - PCAcoord[19:24,3]
   PCAcoord[19:24,5] <- PCAcoord[19:24,3] - PCAcoord[19:24,3]
   PCAcoord[25:30,5] <- PCAcoord[25:30,3] - PCAcoord[19:24,3]
   PCAcoord[31:36,5] <- PCAcoord[31:36,3] - PCAcoord[19:24,3]
   
   PCA1 <- rbind(PCA1, PCAcoord)

   CA <- stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 4004, mfw.max = 4004,
      gui = FALSE)

   CAdist <- as.data.frame(CA$distance.table)
   CAdist$STRAIN <- rownames(CAdist)
   CAdist %>% 
      pivot_longer(-STRAIN, names_to = "sample", values_to = "dist") -> CAdist
   
   CAdist <- CAdist[c(2:6,9:12,16:18,34,35),]
   
   CA1 <- rbind(CA1, CAdist)
   
}
```

OPTION 1
write out iterations
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/")
write.table(PCA1, file = 'PCA_RandomSample_1000_normReadcount_normReadscore.txt')
write.table(CA1, file = 'CA_RandomSample_1000_normReadcount_normReadscore.txt')
```

OPTION 2
write out iterations wit minor and major distances
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/")
write.table(PCA1, file = 'PCA_RandomSample_WithDist_1000_normReadcount_normReadscore.txt')
#write.table(CA1, file = 'CA_RandomSample_WithDist_1000_normReadcount_normReadscore.txt')
```

PLOTTING

OPTION 1
read in iterations
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/")
PCAiter <- read.table(file = 'PCA_RandomSample_1000_normReadcount_normReadscore.txt')
CAiter <- read.table(file = 'CA_RandomSample_1000_normReadcount_normReadscore.txt')
```

OPTION 2
read in iterations with minor and major distances
```{r}
setwd("/projects/dumont-lab/uma/CENPA_ChIP/readscore/")
PCAiter <- read.table(file = 'PCA_RandomSample_WithDist_1000_normReadcount_normReadscore.txt')
CAiter <- read.table(file = 'CA_RandomSample_1000_normReadcount_normReadscore.txt')
```


Checking where the top 1000 fits within the distribution of 1000 replicates
```{r}
top1000.corpus <- load.corpus(files = "all", 
                              corpus.dir = "/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_randomization/real",
                              encoding = "UTF-8")
corpus.words.1.grams <- txt.to.features(top1000.corpus, 
                                        ngram.size = 1,
                                        features = "w")
frequency.list <- make.frequency.list(corpus.words.1.grams)
frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)
frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)

PCA <- stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 3876, mfw.max = 3876,
      gui = FALSE)



PCAcoord <- as.data.frame(PCA$pca.coordinates)
PCAcoord$STRAIN <- rownames(PCAcoord)
PCAcoord %>% 
   pivot_longer(-STRAIN, names_to = "sample", values_to = "PCaxis") -> PCAcoord
   
PCAcoord$DistMinor <- 0
PCAcoord[1:6,4] <- PCAcoord[1:6,3] - PCAcoord[25:30,3]
PCAcoord[7:12,4] <- PCAcoord[7:12,3] - PCAcoord[25:30,3]
PCAcoord[13:18,4] <- PCAcoord[13:18,3] - PCAcoord[25:30,3]
PCAcoord[19:24,4] <- PCAcoord[19:24,3] - PCAcoord[25:30,3]
PCAcoord[25:30,4] <- PCAcoord[25:30,3] - PCAcoord[25:30,3]
PCAcoord[31:36,4] <- PCAcoord[31:36,3] - PCAcoord[25:30,3]
   
   
PCAcoord$DistMajor <- 0
PCAcoord[1:6,5] <- PCAcoord[1:6,3] - PCAcoord[19:24,3]
PCAcoord[7:12,5] <- PCAcoord[7:12,3] - PCAcoord[19:24,3]
PCAcoord[13:18,5] <- PCAcoord[13:18,3] - PCAcoord[19:24,3]
PCAcoord[19:24,5] <- PCAcoord[19:24,3] - PCAcoord[19:24,3]
PCAcoord[25:30,5] <- PCAcoord[25:30,3] - PCAcoord[19:24,3]
PCAcoord[31:36,5] <- PCAcoord[31:36,3] - PCAcoord[19:24,3]


CA <- stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 3876, mfw.max = 3876,
      gui = FALSE)

CA <- CA$distance.table

```


Histogram of PCA
PC1

```{r}

septest <- separate(data = PCAiter, col = STRAIN, into = c("strain", "percent","merge","rank2","rank1","subset","replicate"), sep = "_")
septest <- septest[,c(1,7,8,9,10,11)]

```

Plot each PC per strain (difference from minor satellite)
B6
```{r}
septest %>%
   filter(sample == "PC1", strain == "B6") %>%
   filter(DistMinor < -40.20488) %>%
   summarize(count = n()) -> pvalPC1B6

septest %>%
   filter(sample == "PC1", strain == "B6") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -40.20488, colour = c("red")) -> PC1B6

septest %>%
   filter(sample == "PC2", strain == "B6") %>%
   filter(DistMinor > 3.755223) %>%
   summarize(count = n()) -> pvalPC2B6

septest %>%
   filter(sample == "PC2", strain == "B6") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 3.755223, colour = c("red")) -> PC2B6

septest %>%
   filter(sample == "PC3", strain == "B6") %>%
   filter(DistMinor < -51.01594) %>%
   summarize(count = n()) -> pvalPC3B6

septest %>%
   filter(sample == "PC3", strain == "B6") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -51.01594, colour = c("red")) -> PC3B6

septest %>%
   filter(sample == "PC4", strain == "B6") %>%
   filter(DistMinor < -38.85344) %>%
   summarize(count = n()) -> pvalPC4B6

septest %>%
   filter(sample == "PC4", strain == "B6") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -38.85344, colour = c("red")) -> PC4B6

septest %>%
   filter(sample == "PC5", strain == "B6") %>%
   filter(DistMinor > 2.449490) %>%
   summarize(count = n()) -> pvalPC5B6

septest %>%
   filter(sample == "PC5", strain == "B6") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 2.449490, colour = c("red")) -> PC5B6

septest %>%
   filter(sample == "PC6", strain == "B6") %>%
   filter(DistMinor > 0.0000000000009578727) %>%
   summarize(count = n()) -> pvalPC6B6

septest %>%
   filter(sample == "PC6", strain == "B6") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 0.0000000000009578727, colour = c("red")) -> PC6B6
```

CAST
```{r}
septest %>%
   filter(sample == "PC1", strain == "CAST") %>%
   filter(DistMinor > 34.14669) %>%
   summarize(count = n()) -> pvalPC1CAST

septest %>%
   filter(sample == "PC1", strain == "CAST") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 34.14669, colour = c("red")) -> PC1CAST

septest %>%
   filter(sample == "PC2", strain == "CAST") %>%
   filter(DistMinor < -57.13243) %>%
   summarize(count = n()) -> pvalPC2CAST

septest %>%
   filter(sample == "PC2", strain == "CAST") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -57.13243, colour = c("red")) -> PC2CAST

septest %>%
   filter(sample == "PC3", strain == "CAST") %>%
   filter(DistMinor < -1.253728) %>%
   summarize(count = n()) -> pvalPC3CAST

septest %>%
   filter(sample == "PC3", strain == "CAST") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -1.253728, colour = c("red")) -> PC3CAST

septest %>%
   filter(sample == "PC4", strain == "CAST") %>%
   filter(DistMinor < -39.36455) %>%
   summarize(count = n()) -> pvalPC4CAST

septest %>%
   filter(sample == "PC4", strain == "CAST") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -39.36455, colour = c("red")) -> PC4CAST

septest %>%
   filter(sample == "PC5", strain == "CAST") %>%
   filter(DistMinor > 2.449490) %>%
   summarize(count = n()) -> pvalPC5CAST

septest %>%
   filter(sample == "PC5", strain == "CAST") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 2.449490, colour = c("red")) -> PC5CAST

septest %>%
   filter(sample == "PC6", strain == "CAST") %>%
   filter(DistMinor > 0.00000000000009884454) %>%
   summarize(count = n()) -> pvalPC6CAST

septest %>%
   filter(sample == "PC6", strain == "CAST") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 0.00000000000009884454, colour = c("red")) -> PC6CAST
```

LEWES
```{r}
septest %>%
   filter(sample == "PC1", strain == "LEWES") %>%
   filter(DistMinor < -39.12074) %>%
   summarize(count = n()) -> pvalPC1LEWES

septest %>%
   filter(sample == "PC1", strain == "LEWES") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -39.12074, colour = c("red")) -> PC1LEWES

septest %>%
   filter(sample == "PC2", strain == "LEWES") %>%
   filter(DistMinor > 1.798971) %>%
   summarize(count = n()) -> pvalPC2LEWES

septest %>%
   filter(sample == "PC2", strain == "LEWES") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 1.798971, colour = c("red")) -> PC2LEWES

septest %>%
   filter(sample == "PC3", strain == "LEWES") %>%
   filter(DistMinor > 51.88934) %>%
   summarize(count = n()) -> pvalPC3LEWES

septest %>%
   filter(sample == "PC3", strain == "LEWES") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 51.88934, colour = c("red")) -> PC3LEWES

septest %>%
   filter(sample == "PC4", strain == "LEWES") %>%
   filter(DistMinor < -38.82753) %>%
   summarize(count = n()) -> pvalPC4LEWES

septest %>%
   filter(sample == "PC4", strain == "LEWES") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -38.82753, colour = c("red")) -> PC4LEWES

septest %>%
   filter(sample == "PC5", strain == "LEWES") %>%
   filter(DistMinor > 2.449490) %>%
   summarize(count = n()) -> pvalPC5LEWES

septest %>%
   filter(sample == "PC5", strain == "LEWES") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 2.449490, colour = c("red")) -> PC5LEWES

septest %>%
   filter(sample == "PC6", strain == "LEWES") %>%
   filter(DistMinor < -0.0000000000008004847) %>%
   summarize(count = n()) -> pvalPC6LEWES

septest %>%
   filter(sample == "PC6", strain == "LEWES") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -0.0000000000008004847, colour = c("red")) -> PC6LEWES
```

PWK
```{r}
septest %>%
   filter(sample == "PC1", strain == "PWK") %>%
   filter(DistMinor > 41.99501) %>%
   summarize(count = n()) -> pvalPC1PWK

septest %>%
   filter(sample == "PC1", strain == "PWK") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 41.99501, colour = c("red")) -> PC1PWK

septest %>%
   filter(sample == "PC2", strain == "PWK") %>%
   filter(DistMinor > 51.72426) %>%
   summarize(count = n()) -> pvalPC2PWK

septest %>%
   filter(sample == "PC2", strain == "PWK") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 51.72426, colour = c("red")) -> PC2PWK

septest %>%
   filter(sample == "PC3", strain == "PWK") %>%
   filter(DistMinor > 0.5143351) %>%
   summarize(count = n()) -> pvalPC3PWK

septest %>%
   filter(sample == "PC3", strain == "PWK") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 0.5143351, colour = c("red")) -> PC3PWK

septest %>%
   filter(sample == "PC4", strain == "PWK") %>%
   filter(DistMinor < -39.38281) %>%
   summarize(count = n()) -> pvalPC4PWK

septest %>%
   filter(sample == "PC4", strain == "PWK") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -39.38281, colour = c("red")) -> PC4PWK

septest %>%
   filter(sample == "PC5", strain == "PWK") %>%
   filter(DistMinor > 2.449490) %>%
   summarize(count = n()) -> pvalPC5PWK

septest %>%
   filter(sample == "PC5", strain == "PWK") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 2.449490, colour = c("red")) -> PC5PWK

septest %>%
   filter(sample == "PC6", strain == "PWK") %>%
   filter(DistMinor < -0.000000000001658357) %>%
   summarize(count = n()) -> pvalPC6PWK

septest %>%
   filter(sample == "PC6", strain == "PWK") %>%
   ggplot(aes(x = DistMinor)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = -0.000000000001658357, colour = c("red")) -> PC6PWK
```

```{r}
ggarrange(PC1B6, PC2B6, PC3B6, PC4B6, PC5B6, PC6B6, PC1CAST, PC2CAST, PC3CAST, PC4CAST, PC5CAST, PC6CAST, PC1LEWES, PC2LEWES, PC3LEWES, PC4LEWES, PC5LEWES, PC6LEWES, PC1PWK, PC2PWK, PC3PWK, PC4PWK, PC5PWK, PC6PWK,
          labels = c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6","PC1", "PC2", "PC3", "PC4", "PC5", "PC6","PC1", "PC2", "PC3", "PC4", "PC5", "PC6","PC1", "PC2", "PC3", "PC4", "PC5", "PC6"),
          ncol = 6, nrow = 4)
```


Histogram of CA

```{r}

CAiter_sep <- separate(data = CAiter, col = STRAIN, into = c("strain", "percent","merge","rank2","rank1","subset","replicate"), sep = "_")

CAiter_sep <- CAiter_sep[,c(1,7,8,9)]

CAiter_sep <- separate(data = CAiter_sep, col = sample, into = c("strain2", "percent","merge","rank2","rank1","subset","replicate"), sep = "_")

CAiter_sep <- CAiter_sep[,c(1,2,3)]
```

Make histograms of correlations

```{r}
CAiter_sep %>%
   filter(strain == "B6", strain2 == "CAST") %>%
   filter(dist > 1.2469855) %>%
   summarize(count = n()) -> pvalB6CAST

CAiter_sep %>%
   filter(strain == "B6", strain2 == "CAST") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 1.2469855, colour = c("red")) -> B6CAST

CAiter_sep %>%
   filter(strain == "B6", strain2 == "LEWES") %>%
   filter(dist < 1.1165170) %>%
   summarize(count = n()) -> pvalB6LEWES

CAiter_sep %>%
   filter(strain == "B6", strain2 == "LEWES") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 1.1165170, colour = c("red")) -> B6LEWES

CAiter_sep %>%
   filter(strain == "B6", strain2 == "PWK") %>%
   filter(dist > 1.2465157) %>%
   summarize(count = n()) -> pvalB6PWK

CAiter_sep %>%
   filter(strain == "B6", strain2 == "PWK") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 1.2465157, colour = c("red")) -> B6PWK

CAiter_sep %>%
   filter(strain == "B6", strain2 == "minor") %>%
   filter(dist > 0.617741989) %>%
   summarize(count = n()) -> pvalB6minor

CAiter_sep %>%
   filter(strain == "B6", strain2 == "minor") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 0.617741989, colour = c("red")) -> B6minor

CAiter_sep %>%
   filter(strain == "B6", strain2 == "major") %>%
   filter(dist > 0.617741989) %>%
   summarize(count = n()) -> pvalB6major

CAiter_sep %>%
   filter(strain == "B6", strain2 == "major") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 0.617741989, colour = c("red")) -> B6major

CAiter_sep %>%
   filter(strain == "CAST", strain2 == "LEWES") %>%
   filter(dist > 1.2414600) %>%
   summarize(count = n()) -> pvalCASTLEWES

CAiter_sep %>%
   filter(strain == "CAST", strain2 == "LEWES") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 1.2414600, colour = c("red")) -> CASTLEWES

CAiter_sep %>%
   filter(strain == "CAST", strain2 == "PWK") %>%
   filter(dist > 1.2555489) %>%
   summarize(count = n()) -> pvalCASTPWK

CAiter_sep %>%
   filter(strain == "CAST", strain2 == "PWK") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 1.2555489, colour = c("red")) -> CASTPWK

CAiter_sep %>%
   filter(strain == "CAST", strain2 == "minor") %>%
   filter(dist > 0.631771348) %>%
   summarize(count = n()) -> pvalCASTminor

CAiter_sep %>%
   filter(strain == "CAST", strain2 == "minor") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 0.631771348, colour = c("red")) -> CASTminor

CAiter_sep %>%
   filter(strain == "LEWES", strain2 == "PWK") %>%
   filter(dist > 0.631771348) %>%
   summarize(count = n()) -> pvalLEWESPWK

CAiter_sep %>%
   filter(strain == "LEWES", strain2 == "PWK") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 0.631771348, colour = c("red")) -> LEWESPWK

CAiter_sep %>%
   filter(strain == "LEWES", strain2 == "minor") %>%
   filter(dist > 0.617212579) %>%
   summarize(count = n()) -> pvalLEWESminor

CAiter_sep %>%
   filter(strain == "LEWES", strain2 == "minor") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 0.617212579, colour = c("red")) -> LEWESminor

CAiter_sep %>%
   filter(strain == "PWK", strain2 == "minor") %>%
   filter(dist > 0.632300757) %>%
   summarize(count = n()) -> pvalPWKminor

CAiter_sep %>%
   filter(strain == "PWK", strain2 == "minor") %>%
   ggplot(aes(x = dist)) +
   geom_histogram(bins = 100) +
   theme_classic() +
   geom_vline(xintercept = 0.632300757, colour = c("red")) -> PWKminor

```


```{r}
ggarrange(B6CAST, B6LEWES, B6PWK, B6minor, CASTLEWES, CASTPWK, CASTminor, LEWESPWK, LEWESminor, PWKminor,
          labels = c("B6 v CAST", "B6 v LEWES", "B6 v PWK", "B6 v Minor", "CAST v LEWES", "CAST v PWK","CAST v Minor", "LEWES v PWK", "LEWES v Minor", "PWK v Minor"),
          ncol = 4, nrow = 4)
```



#####stylo w = 1 and without consensus sequence

#########
FEATURES
#########
we use 1 grams (words)

```{r}
top1000.corpus <- load.corpus(files = "all", 
                              corpus.dir = "/projects/dumont-lab/uma/CENPA_ChIP/readscore/stylo_raw/wo_consensus",
                              encoding = "UTF-8")
```


```{r}

corpus.words.1.grams <- txt.to.features(top1000.corpus, 
                                        ngram.size = 1,
                                        features = "w")

```

These functions can produce a summary of all the n-grams in the form of a frequency list, 
which is then used to make a frequency table
```{r}
frequency.list <- make.frequency.list(corpus.words.1.grams)

frequency.list.value <- make.frequency.list(corpus.words.1.grams, value = TRUE)

```


This functions can produce a summary of all the n-grams in the form of a frequency table
produces a stylo.data object
```{r}
frequency.list.table <- make.table.of.frequencies(corpus.words.1.grams, features = frequency.list)
```

```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "PCR",
      custom.graph.title = "Strain Variation",
      mfw.min = 3872, mfw.max = 3872,
      write.png.file = TRUE, 
      gui = FALSE)
```


```{r}

stylo(frequencies = frequency.list.table, 
      analysis.type = "CA",
      custom.graph.title = "Strain Variation Cluster",
      mfw.min = 3872, mfw.max = 3872,
      gui = FALSE)
```

```{r}
stylo(frequencies = frequency.list.table, 
      analysis.type = "BCT",
      custom.graph.title = "Strain Variation Tree",
      mfw.min = 4329, mfw.max = 5329,
      write.png.file = TRUE, 
      gui = FALSE)
```
